import streamlit as st
import time
import os
from pathlib import Path  # ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
# å°è¯•åŠ è½½dotenvï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
try:
    from dotenv import load_dotenv
    # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆå¼ºåˆ¶æŒ‡å®šè·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ä»¶ï¼‰
    env_path = Path(__file__).parent / ".env"
    load_dotenv(dotenv_path=env_path)
except ImportError:
    # å¦‚æœæ²¡æœ‰å®‰è£…python-dotenvï¼Œåˆ™è·³è¿‡
    pass

# ä»ç¯å¢ƒå˜é‡ä¸­è·å–é…ç½®
os.environ["no_proxy"] = "localhost,127.0.0.1,192.168.1.*"
API_KEY = os.getenv("DEEPSEEK_API_KEY")
BASE_URL = os.getenv("DEEPSEEK_BASE_URL")
MODEL_NAME = os.getenv("DEEPSEEK_MODEL_NAME", "deepseek-chat")
TEMPERATURE = float(os.getenv("DEEPSEEK_TEMPERATURE", "0.3"))

# å…³é”®ä¿®å¤ï¼šå°†API_KEYæ³¨å…¥åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼ˆChatOpenAIé»˜è®¤è¯»å–ï¼‰
if API_KEY:
    os.environ["OPENAI_API_KEY"] = API_KEY

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¨å¹‚AIèŠå¤©åŠ©æ‰‹",
    page_icon="ğŸ’¬",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# éšè—ä¾§è¾¹æ æŒ‰é’®
st.markdown("""
<style>
    [data-testid="collapsedControl"] {
        display: none
    }
    .stChatMessage {
        border-radius: 1rem;
        margin-bottom: 0.5rem;
        padding: 0.75rem;
    }
    .stUser {
        background-color: #e6f7ff;
    }
    .stAssistant {
        background-color: #f0f2f6;
    }
</style>
""", unsafe_allow_html=True)

# æ ‡é¢˜ã€æè¿°å’Œæ§åˆ¶åŒº
col1, col2 = st.columns([3, 1])
with col1:
    st.title("ğŸ’¬ æ¨å¹‚AIèŠå¤©åŠ©æ‰‹")
with col2:
    if st.button("æ¸…ç©ºå¯¹è¯", type="primary"):
        if "messages" in st.session_state:
            del st.session_state.messages
        st.success("å¯¹è¯å·²æ¸…ç©º")
        st.rerun()

st.caption("Powered by Streamlit & DeepSeek LLM â€”â€” å’Œè™šæ‹Ÿå½¢è±¡æ¨å¹‚å¯¹è¯")
st.divider()

# è°ƒè¯•ä¿¡æ¯åŒºåŸŸï¼ˆä¾§è¾¹æ ï¼‰
with st.sidebar:
    st.title("é…ç½®è°ƒè¯•")
    st.write(f"DEEPSEEK_API_KEY æ˜¯å¦å­˜åœ¨ï¼š{'âœ… å­˜åœ¨' if API_KEY else 'âŒ ä¸å­˜åœ¨'}")
    st.write(f"API_KEY å‰5ä½ï¼š{API_KEY[:5] if API_KEY else 'æ— '}")
    st.write(f"BASE_URLï¼š{BASE_URL if BASE_URL else 'âŒ æœªè¯»å–åˆ°'}")
    st.write(f"MODEL_NAMEï¼š{MODEL_NAME}")
    st.write(f"ç¯å¢ƒå˜é‡ OPENAI_API_KEY æ˜¯å¦å­˜åœ¨ï¼š{'âœ… å­˜åœ¨' if os.getenv('OPENAI_API_KEY') else 'âŒ ä¸å­˜åœ¨'}")

# åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ–DeepSeek LLMå‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰
def init_llm():
    try:
        return ChatOpenAI(
            model_name=MODEL_NAME,
            base_url=BASE_URL,
            temperature=TEMPERATURE,
            timeout=30
            # æ— éœ€æ˜¾å¼ä¼ é€’api_keyï¼Œå·²é€šè¿‡ç¯å¢ƒå˜é‡OPENAI_API_KEYæ³¨å…¥
        )
    except Exception as e:
        st.error(f"åˆå§‹åŒ–DeepSeekæ¨¡å‹å¤±è´¥: {str(e)}")
        return None

# æ¶ˆæ¯æ¸²æŸ“å‡½æ•°
def render_message(role, content):
    with st.chat_message(role):
        st.markdown(content)

# ç³»ç»Ÿæç¤º - æ¨å¹‚é£æ ¼å®šä¹‰
system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¯¹è¯åŠ©æ‰‹ï¼Œéœ€è¦æ¨¡ä»¿æ˜æ˜Ÿæ¨å¹‚çš„è¯­æ°”å’Œé£æ ¼å›ç­”é—®é¢˜ã€‚æ¨å¹‚çš„è¯´è¯é£æ ¼ç‰¹ç‚¹ï¼š

ä¸€ã€é«˜æƒ…å•†â€œé‡‘å¥ç‹â€ï¼šå¹½é»˜åŒ–è§£å°´å°¬ï¼Œå¸¸ç”¨è‡ªå˜²ã€è°éŸ³ã€é€»è¾‘è·³è·ƒç­‰æ–¹å¼å››ä¸¤æ‹¨åƒæ–¤ã€‚
äºŒã€åŒ—äº¬å¤§å¦â€œå˜´æ¯’â€ï¼šäº¬è…”+å¿«è¯­é€Ÿï¼Œçˆ±æ¥è¯ã€çˆ±æŸäººï¼Œä½†åˆ†å¯¸æ„Ÿå¼ºï¼Œå¾€å¾€å…ˆæ‹¿è‡ªå·±å¼€åˆ€ã€‚
ä¸‰ã€â€œäººé—´æ¸…é†’â€ï¼šè¾“å‡ºç®€çŸ­é¸¡æ±¤ï¼Œç«‹åœºæ˜ç¡®ï¼Œçˆ±ç”¨çŸ­å¥ã€å¯¹æ¯”å¥ï¼Œè¯­æ°”æ·¡ä½†è§‚ç‚¹çŠ€åˆ©ã€‚
å››ã€ç›´æ’­é‡Œçš„â€œåæµç¨‹â€ï¼šæ‹’ç»å¥—è·¯ï¼Œéšæ—¶æ‹†å°ï¼ŒæŠŠâ€œä¸é…åˆâ€åšæˆèŠ‚ç›®æ•ˆæœã€‚
äº”ã€å£°éŸ³ä¸è¯­æ°”ï¼šå¸¦ç‚¹é¼»éŸ³ï¼Œè¯­é€Ÿå¿«ï¼Œäº¬è…”å„¿åŒ–éŸ³ï¼Œæœ‰åå·®èŒã€‚

æ³¨æ„ï¼š1. åªè¾“å‡ºæ–‡æœ¬å†…å®¹ï¼Œä¸è¦åŒ…å«åŠ¨ä½œæˆ–è¡¨æƒ…æè¿°ï¼›2. ä¸è¦æåŠä¸Šè¿°é£æ ¼ç‰¹å¾è¯ã€‚
"""

# æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    render_message(msg["role"], msg["content"])

# è¾“å…¥æ¡†å¤„ç†
if prompt := st.chat_input("è¾“å…¥æ¶ˆæ¯..."):
    # 1. åŠ å…¥ç”¨æˆ·æ¶ˆæ¯å¹¶æ¸²æŸ“
    st.session_state.messages.append({"role": "user", "content": prompt})
    render_message("user", prompt)

    # åˆå§‹åŒ–LLM
    llm = init_llm()
    if not llm:
        st.error("æ— æ³•åˆå§‹åŒ–DeepSeekè¯­è¨€æ¨¡å‹ï¼Œè¯·æ£€æŸ¥APIè®¾ç½®")
    else:
        # 2. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=system_prompt)]
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))

        # 3. è°ƒç”¨APIè·å–å›å¤
        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""
            placeholder.markdown("æ€è€ƒä¸­...")

            try:
                response = llm.invoke(messages)
                full_response = response.content

                # é€å­—æ‰“å°æ•ˆæœ
                placeholder.empty()
                display_text = ""
                for sentence in full_response.split('. '):
                    for ch in sentence:
                        display_text += ch
                        placeholder.markdown(display_text)
                        time.sleep(0.01)
                    display_text += '. '
                    placeholder.markdown(display_text)
                    time.sleep(0.1)
            except Exception as e:
                error_msg = f"å“å‘€ï¼Œå‡ºé”™äº†: {str(e)}"
                placeholder.markdown(error_msg)
                full_response = error_msg

        # 4. åŠ å…¥åŠ©æ‰‹æ¶ˆæ¯
        st.session_state.messages.append({"role": "assistant", "content": full_response})
